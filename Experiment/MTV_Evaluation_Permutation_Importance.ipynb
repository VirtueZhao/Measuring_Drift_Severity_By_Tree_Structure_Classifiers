{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MySkmultiflow.data import DataStream\n",
    "from MySkmultiflow.trees import HoeffdingTreeClassifier\n",
    "from statistics import mean, stdev\n",
    "import csv\n",
    "import pandas as pd\n",
    "import eli5\n",
    "import MTVlib\n",
    "\n",
    "def Get_Permutation_Importance(Model,DataStream):\n",
    "    print(\"Get Permutation Importance\")\n",
    "    PermutationImportance = eli5.sklearn.PermutationImportance(Model,random_state=42).fit(DataStream.X, DataStream.y)\n",
    "    PermutationImportance_Means = PermutationImportance.feature_importances_\n",
    "    \n",
    "    return PermutationImportance_Means\n",
    "\n",
    "def Permutation_Importance_Feature(Permutation_Importance, Feature_List):\n",
    "    Permutation_Importance_Sum = []\n",
    "    for feature in Feature_List:\n",
    "        Permutation_Importance_Sum.append(Permutation_Importance[int(feature)])\n",
    "    \n",
    "    return sum(Permutation_Importance_Sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reverse Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Adult_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '59'\n",
    "    count = len(dataframe[target_index])\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_reverse(dataframe, target_index, i)\n",
    "    \n",
    "    target_idx = 59\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "    \n",
    "    Permutation_Importance_Base = sum(Get_Permutation_Importance(HT_Base, Stream_Base))\n",
    "    Permutation_Importance_1_1 = sum(Get_Permutation_Importance(HT_1_1, Stream_1_1))\n",
    "    Permutation_Importance_1_2 = sum(Get_Permutation_Importance(HT_1_2, Stream_1_2))\n",
    "    Permutation_Importance_1_4 = sum(Get_Permutation_Importance(HT_1_4, Stream_1_4))\n",
    "    Permutation_Importance_1_8 = sum(Get_Permutation_Importance(HT_1_8, Stream_1_8))\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Reverse_Adult'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reverse Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Bank_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '41'\n",
    "    count = len(dataframe[target_index])\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_reverse(dataframe, target_index, i)\n",
    "    \n",
    "    target_idx = 41\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "    \n",
    "    Permutation_Importance_Base = sum(Get_Permutation_Importance(HT_Base, Stream_Base))\n",
    "    Permutation_Importance_1_1 = sum(Get_Permutation_Importance(HT_1_1, Stream_1_1))\n",
    "    Permutation_Importance_1_2 = sum(Get_Permutation_Importance(HT_1_2, Stream_1_2))\n",
    "    Permutation_Importance_1_4 = sum(Get_Permutation_Importance(HT_1_4, Stream_1_4))\n",
    "    Permutation_Importance_1_8 = sum(Get_Permutation_Importance(HT_1_8, Stream_1_8))\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Reverse_Bank'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reverse Cardio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Cardio_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '10'\n",
    "    count = len(dataframe[target_index])\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_reverse(dataframe, target_index, i)\n",
    "    \n",
    "    target_idx = 10\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "    \n",
    "    Permutation_Importance_Base = sum(Get_Permutation_Importance(HT_Base, Stream_Base))\n",
    "    Permutation_Importance_1_1 = sum(Get_Permutation_Importance(HT_1_1, Stream_1_1))\n",
    "    Permutation_Importance_1_2 = sum(Get_Permutation_Importance(HT_1_2, Stream_1_2))\n",
    "    Permutation_Importance_1_4 = sum(Get_Permutation_Importance(HT_1_4, Stream_1_4))\n",
    "    Permutation_Importance_1_8 = sum(Get_Permutation_Importance(HT_1_8, Stream_1_8))\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Reverse_Cardio'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reverse Chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Chess_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '40'\n",
    "    count = len(dataframe[target_index])\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_reverse(dataframe, target_index, i)\n",
    "    \n",
    "    target_idx = 40\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "    \n",
    "    Permutation_Importance_Base = sum(Get_Permutation_Importance(HT_Base, Stream_Base))\n",
    "    Permutation_Importance_1_1 = sum(Get_Permutation_Importance(HT_1_1, Stream_1_1))\n",
    "    Permutation_Importance_1_2 = sum(Get_Permutation_Importance(HT_1_2, Stream_1_2))\n",
    "    Permutation_Importance_1_4 = sum(Get_Permutation_Importance(HT_1_4, Stream_1_4))\n",
    "    Permutation_Importance_1_8 = sum(Get_Permutation_Importance(HT_1_8, Stream_1_8))\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Reverse_Chess'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reverse Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Credit_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '26'\n",
    "    count = len(dataframe[target_index])\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_reverse(dataframe, target_index, i)\n",
    "    \n",
    "    target_idx = 26\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "    \n",
    "    Permutation_Importance_Base = sum(Get_Permutation_Importance(HT_Base, Stream_Base))\n",
    "    Permutation_Importance_1_1 = sum(Get_Permutation_Importance(HT_1_1, Stream_1_1))\n",
    "    Permutation_Importance_1_2 = sum(Get_Permutation_Importance(HT_1_2, Stream_1_2))\n",
    "    Permutation_Importance_1_4 = sum(Get_Permutation_Importance(HT_1_4, Stream_1_4))\n",
    "    Permutation_Importance_1_8 = sum(Get_Permutation_Importance(HT_1_8, Stream_1_8))\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Reverse_Credit'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reverse Diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Diamonds_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '20'\n",
    "    count = len(dataframe[target_index])\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_reverse(dataframe, target_index, i)\n",
    "    \n",
    "    target_idx = 20\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "    \n",
    "    Permutation_Importance_Base = sum(Get_Permutation_Importance(HT_Base, Stream_Base))\n",
    "    Permutation_Importance_1_1 = sum(Get_Permutation_Importance(HT_1_1, Stream_1_1))\n",
    "    Permutation_Importance_1_2 = sum(Get_Permutation_Importance(HT_1_2, Stream_1_2))\n",
    "    Permutation_Importance_1_4 = sum(Get_Permutation_Importance(HT_1_4, Stream_1_4))\n",
    "    Permutation_Importance_1_8 = sum(Get_Permutation_Importance(HT_1_8, Stream_1_8))\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Reverse_Diamonds'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reverse Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Gamma_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '10'\n",
    "    count = len(dataframe[target_index])\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_reverse(dataframe, target_index, i)\n",
    "    \n",
    "    target_idx = 10\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "    \n",
    "    Permutation_Importance_Base = sum(Get_Permutation_Importance(HT_Base, Stream_Base))\n",
    "    Permutation_Importance_1_1 = sum(Get_Permutation_Importance(HT_1_1, Stream_1_1))\n",
    "    Permutation_Importance_1_2 = sum(Get_Permutation_Importance(HT_1_2, Stream_1_2))\n",
    "    Permutation_Importance_1_4 = sum(Get_Permutation_Importance(HT_1_4, Stream_1_4))\n",
    "    Permutation_Importance_1_8 = sum(Get_Permutation_Importance(HT_1_8, Stream_1_8))\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Reverse_Gamma'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Reverse PokerPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_PokerPart_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '85'\n",
    "    count = len(dataframe[target_index])\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_reverse(dataframe, target_index, i)\n",
    "    \n",
    "    target_idx = 85\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "    \n",
    "    Permutation_Importance_Base = sum(Get_Permutation_Importance(HT_Base, Stream_Base))\n",
    "    Permutation_Importance_1_1 = sum(Get_Permutation_Importance(HT_1_1, Stream_1_1))\n",
    "    Permutation_Importance_1_2 = sum(Get_Permutation_Importance(HT_1_2, Stream_1_2))\n",
    "    Permutation_Importance_1_4 = sum(Get_Permutation_Importance(HT_1_4, Stream_1_4))\n",
    "    Permutation_Importance_1_8 = sum(Get_Permutation_Importance(HT_1_8, Stream_1_8))\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Reverse_PokerPart'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 0 Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Working on Interation: 1\n",
      "generate dataframe for feature drift 0\n",
      "Get Permutation Importance\n",
      "Get Permutation Importance\n"
     ]
    }
   ],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Adult_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '59'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 0\n",
    "    \n",
    "    feature_shift_list = ['24', '47', '26', '46', '56', '57', '35', '48', '33', '8', '36', '18', '0', '50']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 59\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_0_Adult'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 0 Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Bank_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '41'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 0\n",
    "    \n",
    "    feature_shift_list = ['37', '40', '24', '22', '1', '29', '33', '34', '27', '19']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 41\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_0_Bank'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 0 Cardio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Cardio_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '10'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 0\n",
    "    \n",
    "    feature_shift_list = ['4', '6', '7']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 10\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_0_Cardio'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 0 Chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Chess_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '40'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 0\n",
    "    \n",
    "    feature_shift_list = ['32', '4', '7', '6', '3', '24', '33', '1', '36', '27']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 40\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_0_Chess'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 0 Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Credit_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '26'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 0\n",
    "    \n",
    "    feature_shift_list = ['0', '7', '4', '6', '19', '18']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 26\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_0_Credit'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 0 Diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Diamonds_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '20'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 0\n",
    "    \n",
    "    feature_shift_list = ['12', '0', '5', '6', '17']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 20\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_0_Diamonds'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 0 Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Gamma_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '10'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 0\n",
    "    \n",
    "    feature_shift_list = ['8', '0', '1']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 10\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_0_Gamma'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 0 PokerPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_PokerPart_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '85'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 0\n",
    "    \n",
    "    feature_shift_list = ['13', '46', '54', '27', '26', '22', '65', '25', '9', '42', '20', '8', '5', '72', '43', '41', '6',\n",
    "                         '1', '2', '29', '80']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 85\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_0_PokerPart'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 1 Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Adult_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '59'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 1\n",
    "    \n",
    "    feature_shift_list = ['24', '47', '26', '46', '56', '57', '35', '48', '33', '8', '36', '18', '0', '50']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 59\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_1_Adult'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 1 Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Bank_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '41'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 1\n",
    "    \n",
    "    feature_shift_list = ['37', '40', '24', '22', '1', '29', '33', '34', '27', '19']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 41\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_1_Bank'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 1 Cardio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Cardio_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '10'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 1\n",
    "    \n",
    "    feature_shift_list = ['4', '6', '7']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 10\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_1_Cardio'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 1 Chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Chess_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '40'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 1\n",
    "    \n",
    "    feature_shift_list = ['32', '4', '7', '6', '3', '24', '33', '1', '36', '27']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 40\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_1_Chess'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 1 Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Credit_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '26'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 1\n",
    "    \n",
    "    feature_shift_list = ['0', '7', '4', '6', '19', '18']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 26\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_1_Credit'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 1 Diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Diamonds_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '20'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 1\n",
    "    \n",
    "    feature_shift_list = ['12', '0', '5', '6', '17']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 20\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_1_Diamonds'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature 1 Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_Gamma_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '10'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 1\n",
    "    \n",
    "    feature_shift_list = ['8', '0', '1']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 10\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_1_Gamma'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1 PokerPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iteration = 3\n",
    "Permutation_Importance_base_all = []\n",
    "Permutation_Importance_1_all = []\n",
    "Permutation_Importance_2_all = []\n",
    "Permutation_Importance_4_all = []\n",
    "Permutation_Importance_8_all = []\n",
    "\n",
    "for i in range(Iteration):\n",
    "    print(\"Currently Working on Interation: \" + str(i+1))\n",
    "\n",
    "    dataframe = pd.read_csv('MTV_Datasets/Preprocessed_Datasets/Synthetic_PokerPart_Transformed.csv', header=None)\n",
    "    dataframe.columns = dataframe.columns.astype(str)\n",
    "    target_index = '85'\n",
    "    count = len(dataframe[target_index])\n",
    "    drift_type = 1\n",
    "    \n",
    "    feature_shift_list = ['13', '46', '54', '27', '26', '22', '65', '25', '9', '42', '20', '8', '5', '72', '43', '41', '6',\n",
    "                         '1', '2', '29', '80']\n",
    "    \n",
    "    dataframe_base, dataframe_1_1, dataframe_1_2, dataframe_1_4, dataframe_1_8 = MTVlib.generate_dataframe_feature(dataframe, target_index, feature_shift_list, 0, drift_type)\n",
    "    \n",
    "    target_idx = 85\n",
    "    cat_features= list(range(target_idx))\n",
    "    \n",
    "    Stream_Base = DataStream(dataframe_base, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_1 = DataStream(dataframe_1_1, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_2 = DataStream(dataframe_1_2, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_4 = DataStream(dataframe_1_4, target_idx=target_idx, cat_features=cat_features)\n",
    "    Stream_1_8 = DataStream(dataframe_1_8, target_idx=target_idx, cat_features=cat_features)\n",
    "    \n",
    "    HT_Base = HoeffdingTreeClassifier(binary_split=True, no_preprune=True)\n",
    "    HT_Base.partial_fit(Stream_Base.X, Stream_Base.y)\n",
    "\n",
    "    constrain_dict = HT_Base.constrain_dict\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "\n",
    "    HT_1_1 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_1.partial_fit(Stream_1_1.X, Stream_1_1.y)\n",
    "    feature_list_base_1 = HT_1_1.feature_list\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_2 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_2.partial_fit(Stream_1_2.X, Stream_1_2.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_4 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_4.partial_fit(Stream_1_4.X, Stream_1_4.y)\n",
    "\n",
    "    Base_feature_list = HT_Base.feature_list.copy()\n",
    "    HT_1_8 = HoeffdingTreeClassifier(constrain_dict=constrain_dict, feature_list=Base_feature_list)\n",
    "    HT_1_8.partial_fit(Stream_1_8.X, Stream_1_8.y)\n",
    "\n",
    "    Permutation_Importance_Base = Permutation_Importance_Feature(Get_Permutation_Importance(HT_Base, Stream_Base), feature_shift_list)\n",
    "    Permutation_Importance_1_1 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_1, Stream_1_1), feature_shift_list)\n",
    "    Permutation_Importance_1_2 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_2, Stream_1_2), feature_shift_list)\n",
    "    Permutation_Importance_1_4 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_4, Stream_1_4), feature_shift_list)\n",
    "    Permutation_Importance_1_8 = Permutation_Importance_Feature(Get_Permutation_Importance(HT_1_8, Stream_1_8), feature_shift_list)\n",
    "    \n",
    "    Permutation_Importance_base_all.append(Permutation_Importance_Base)\n",
    "    Permutation_Importance_1_all.append(Permutation_Importance_1_1)\n",
    "    Permutation_Importance_2_all.append(Permutation_Importance_1_2)\n",
    "    Permutation_Importance_4_all.append(Permutation_Importance_1_4)\n",
    "    Permutation_Importance_8_all.append(Permutation_Importance_1_8)\n",
    "\n",
    "with open('MTV_Evaluation/Evaluation_PermutationImportance.csv', 'a', newline='') as csv_file:\n",
    "    dataset = 'Feature_1_PokerPart'\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "    mean_base = mean(Permutation_Importance_base_all)\n",
    "    mean_1 = mean(Permutation_Importance_1_all)\n",
    "    mean_2 = mean(Permutation_Importance_2_all)\n",
    "    mean_4 = mean(Permutation_Importance_4_all)\n",
    "    mean_8 = mean(Permutation_Importance_8_all)\n",
    "    \n",
    "    stdev_base = stdev(Permutation_Importance_base_all)\n",
    "    stdev_1 = stdev(Permutation_Importance_1_all)\n",
    "    stdev_2 = stdev(Permutation_Importance_2_all)\n",
    "    stdev_4 = stdev(Permutation_Importance_4_all)\n",
    "    stdev_8 = stdev(Permutation_Importance_8_all)\n",
    "    \n",
    "    r_b = str(round(mean_base, 4)) + \" $\\pm$ \" + str(round(stdev_base, 4))\n",
    "    r_1 = str(round(mean_1, 4)) + \" $\\pm$ \" + str(round(stdev_1, 4))\n",
    "    r_2 = str(round(mean_2, 4)) + \" $\\pm$ \" + str(round(stdev_2, 4))\n",
    "    r_4 = str(round(mean_4, 4)) + \" $\\pm$ \" + str(round(stdev_4, 4))\n",
    "    r_8 = str(round(mean_8, 4)) + \" $\\pm$ \" + str(round(stdev_8, 4))\n",
    "    \n",
    "    writer.writerow([dataset, r_b, r_1, r_2, r_4, r_8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
